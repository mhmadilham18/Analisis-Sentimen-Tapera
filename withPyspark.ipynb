{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library yang diperlukan\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "from pyspark.ml.classification import NaiveBayes, LinearSVC\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inisialisasi SparkSession\n",
    "spark = SparkSession.builder.appName(\"TextAnalysisWithSpark\").getOrCreate()\n",
    "\n",
    "# Memulai waktu eksekusi\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to load dataset: 0.3029024600982666 seconds\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "directory = 'hasil-crawling'  # Sesuaikan dengan direktori tempat dataset Anda\n",
    "df = spark.read.csv('hasil-crawling.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Logging waktu untuk load dataset\n",
    "load_dataset_time = time.time() - start_time\n",
    "print(f\"Time taken to load dataset: {load_dataset_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocessing menggunakan UDF di Spark\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import WordNetLemmatizer\n",
    "import re\n",
    "from deep_translator import GoogleTranslator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Inisialisasi translator\n",
    "translator = GoogleTranslator(source='auto', target='en')\n",
    "\n",
    "# Fungsi untuk membersihkan teks\n",
    "def clean_text(text):\n",
    "    # Translate\n",
    "    text = translator.translate(text)\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub('[^a-zA-Z\\s]', '', text)\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    text = ' '.join([stemmer.stem(word) for word in text.split()])\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Register UDF untuk Spark DataFrame\n",
    "clean_text_udf = udf(lambda x: clean_text(x), StringType())\n",
    "df = df.withColumn('tweet_clean', clean_text_udf(col('full_text')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for preprocessing: 0.08727359771728516 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Logging waktu untuk preprocessing\n",
    "preprocessing_time = time.time() - start_time - load_dataset_time\n",
    "print(f\"Time taken for preprocessing: {preprocessing_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisasi teks\n",
    "tokenizer = Tokenizer(inputCol=\"tweet_clean\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(df)\n",
    "\n",
    "# Menghitung TF (Term Frequency) dengan HashingTF\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "\n",
    "# Menghitung IDF (Inverse Document Frequency)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging waktu untuk TF-IDF\n",
    "tfidf_time = time.time() - start_time - load_dataset_time - preprocessing_time\n",
    "print(f\"Time taken for TF-IDF computation: {tfidf_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentimen Analysis menggunakan VADER Sentiment di Spark\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Inisialisasi VADER sentiment analyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Fungsi untuk menentukan label sentimen\n",
    "def vader_sentiment(text):\n",
    "    scores = sid.polarity_scores(text)\n",
    "    if scores['compound'] >= 0.05:\n",
    "        return 'positive'\n",
    "    elif scores['compound'] <= -0.05:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register UDF untuk Spark DataFrame\n",
    "vader_sentiment_udf = udf(lambda x: vader_sentiment(x), StringType())\n",
    "rescaledData = rescaledData.withColumn(\"sentiment\", vader_sentiment_udf(col(\"tweet_clean\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging waktu untuk sentiment analysis\n",
    "sentiment_time = time.time() - start_time - load_dataset_time - preprocessing_time - tfidf_time\n",
    "print(f\"Time taken for sentiment analysis: {sentiment_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split data menjadi training dan testing set\n",
    "(trainingData, testData) = rescaledData.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing label\n",
    "indexer = StringIndexer(inputCol=\"sentiment\", outputCol=\"label\")\n",
    "indexedData = indexer.fit(rescaledData).transform(rescaledData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Naive Bayes model\n",
    "nb = NaiveBayes(featuresCol='features', labelCol='label', modelType='multinomial')\n",
    "\n",
    "# Logging waktu untuk training Naive Bayes\n",
    "start_train_nb_time = time.time()\n",
    "\n",
    "nbModel = nb.fit(trainingData)\n",
    "\n",
    "# Logging waktu untuk training Naive Bayes\n",
    "train_nb_time = time.time() - start_train_nb_time\n",
    "print(f\"Time taken for training Naive Bayes: {train_nb_time} seconds\")\n",
    "\n",
    "# Prediksi menggunakan model Naive Bayes\n",
    "nbPredictions = nbModel.transform(testData)\n",
    "\n",
    "# Evaluasi model Naive Bayes\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "nbAccuracy = evaluator.evaluate(nbPredictions)\n",
    "print(f\"Naive Bayes - Accuracy: {nbAccuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM model\n",
    "svm = LinearSVC(featuresCol='features', labelCol='label')\n",
    "\n",
    "# Logging waktu untuk training SVM\n",
    "start_train_svm_time = time.time()\n",
    "\n",
    "svmModel = svm.fit(trainingData)\n",
    "\n",
    "# Logging waktu untuk training SVM\n",
    "train_svm_time = time.time() - start_train_svm_time\n",
    "print(f\"Time taken for training SVM: {train_svm_time} seconds\")\n",
    "\n",
    "# Prediksi menggunakan model SVM\n",
    "svmPredictions = svmModel.transform(testData)\n",
    "\n",
    "# Evaluasi model SVM\n",
    "svmAccuracy = evaluator.evaluate(svmPredictions)\n",
    "print(f\"SVM - Accuracy: {svmAccuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total waktu eksekusi\n",
    "total_execution_time = time.time() - start_time\n",
    "print(f\"Total execution time: {total_execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualisasi grafik untuk analisis beban\n",
    "stages = ['Load Dataset', 'Text Cleaning', 'TF-IDF Computation', 'Sentiment Analysis', \n",
    "          'Train Naive Bayes', 'Train SVM']\n",
    "times = [load_dataset_time, preprocessing_time, tfidf_time, sentiment_time, \n",
    "         train_nb_time, train_svm_time]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(stages, times, color='skyblue')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.title('Execution Time for Each Stage')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "# Stop SparkSession\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
